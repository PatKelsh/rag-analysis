{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Pipeline, pipeline\n",
    "\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"data/path/to/document/directory\"\n",
    "QUESTIONS = [\n",
    "    \"insert multiple questions here\",\n",
    "    \"some should be relevant to the document use case\",\n",
    "    \"some should be irrelevant to the use case\",\n",
    "    ]\n",
    "EMBEDDING_MODEL_NAME = \"model/path/to/document/parsing/model\" # puts data in the knowledge base\n",
    "READER_MODEL_NAME = \"model/path/to/user/interface/model\" # handles recieving user prompt and generating the response\n",
    "RERANKER_MODEL = \"model/path/to/raranking/model\" # reranks the documents for more relevant documents used in the response\n",
    "CHUNK_SIZE = 512 # Choose a chunk size to best fit the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingest\n",
    "\n",
    "## Initial Data Preperation\n",
    "\n",
    "Ingest a directory of the documents to be turned into a knowledge base. This will result in a Dictionary with each entry having the `text` of each document and its `filepath` for reference later in the response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_markdown_directory(rootdir: str) -> dict:\n",
    "    files_dict = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "\n",
    "            if filepath.endswith(\".md\" or \"mdx\"):\n",
    "                files_dict.append({\n",
    "                    \"text\": Path(filepath).read_text(),\n",
    "                    \"source\" : filepath\n",
    "                })\n",
    "    return files_dict\n",
    "\n",
    "ds = ingest_markdown_directory(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ingest_markdown_directory(DATASET)\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in ds\n",
    "]\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Documents\n",
    "\n",
    "Take the documents and split them into chunks based on the `CHUNK_SIZE` variable. Different chunks can produce different results so this will be analyzed in the next step. These chunks are then run through the embedding model to be converted into document vectors before being loaded into a FAISS vector database which will serve as the knowledge base for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = split_documents(\n",
    "    CHUNK_SIZE,\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitting Analysis\n",
    "\n",
    "Analysis of the document splitting process. As documents a split into chunks, if the splitting is too aggressive chunks won't be long enough to have the nessessary context, but if they are too long then information can get lost in the size of the chunk. Also, most documents should be similar in length so the size of the chunks should appear to be a left skewed bell curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter.\n",
    "print(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in docs_processed]\n",
    "\n",
    "# Plot the distrubution of document lengths, counted as the number of tokens\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Clustering Analysis\n",
    "\n",
    "This looks at how the individual document chunks have been turned into vectors that will be used in responding to a prompt. While not the be all and end all, this analysis can highlight any clusters of documents that may be out of alignment with the rest of the corpus. If that is the case, consider if that set of documents is nessesary for the inteneded use case or if another embedding model should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# embed a user query in the same space\n",
    "query_vectors = [embedding_model.embed_query(question) for question in QUESTIONS]\n",
    "\n",
    "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "embeddings_2d = [\n",
    "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n",
    "]\n",
    "for vector in query_vectors:\n",
    "    embeddings_2d.append(vector)\n",
    "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
    "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")\n",
    "\n",
    "df = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        {\n",
    "            \"x\": documents_projected[i, 0],\n",
    "            \"y\": documents_projected[i, 1],\n",
    "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[3],\n",
    "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
    "            \"symbol\": \"circle\",\n",
    "            \"size_col\": 4,\n",
    "        }\n",
    "        for i in range(len(docs_processed))\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            \"x\": documents_projected[len(docs_processed) + i, 0],\n",
    "            \"y\": documents_projected[len(docs_processed)  +i, 1],\n",
    "            \"source\": \"User query\",\n",
    "            \"extract\": QUESTIONS[i],\n",
    "            \"size_col\": 100,\n",
    "            \"symbol\": \"star\",\n",
    "        }\n",
    "        for i in range(len(QUESTIONS))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# visualize the embedding\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"source\",\n",
    "    hover_data=\"extract\",\n",
    "    size=\"size_col\",\n",
    "    symbol=\"symbol\",\n",
    "    color_discrete_map={\"User query\": \"black\"},\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"<b>Chunk source</b>\",\n",
    "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Generator Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Reader LLM\n",
    "\n",
    "Set the Reader LLM that will parse the prompt and generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full prompt that will be sent to the LLM for all prompts. The given prompt will be inserted into the `question` field while the relevant document chunks will be given to the LLM in the `context` field. Also included is the prompt used when the LLM does not have an attached knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    rag_prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "llm_prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the source when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "Here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "LLM_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    llm_prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    doc_sources = [doc.metadata for doc in relevant_docs]\n",
    "    relevant_docs_content = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        ranked_docs = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n",
    "        # doc_sources = [doc.metadata for doc in relevant_docs]\n",
    "        ranked_sources = []\n",
    "        for doc in ranked_docs:\n",
    "            index = relevant_docs_content.index(doc[\"content\"])\n",
    "            ranked_sources.append(doc_sources[index])\n",
    "        doc_sources = ranked_sources\n",
    "        relevant_docs_content = [doc[\"content\"] for doc in ranked_docs]\n",
    "\n",
    "\n",
    "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_content)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Generate an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
    "\n",
    "    sources = [doc_source['source'] for doc_source in doc_sources[:num_docs_final]]\n",
    "\n",
    "    return answer, sources\n",
    "\n",
    "def answer_with_no_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    ") -> str:\n",
    "    prompt = LLM_PROMPT_TEMPLATE.format(question=question)\n",
    "\n",
    "    # Generate an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Model\n",
    "\n",
    "This is the control version of the model, producing a response only based on the base model. There is no additional context provided by the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_dict = []\n",
    "for question in QUESTIONS:\n",
    "    answer = answer_with_no_rag(question, READER_LLM)\n",
    "    plain_dict.append({\n",
    "        \"prompt\": question,\n",
    "        \"response\": answer,\n",
    "    })\n",
    "plain_df = pd.DataFrame.from_dict(plain_dict)\n",
    "display(plain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Model\n",
    "\n",
    "This model uses the provided documents with the provided model in a RAG system. The response included both the answer to the given question and the document used to source the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dict = []\n",
    "for question in QUESTIONS:\n",
    "    answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE)\n",
    "    rag_dict.append({\n",
    "        \"prompt\": question,\n",
    "        \"response\": answer,\n",
    "        \"sources\": relevant_docs\n",
    "    })\n",
    "rag_df = pd.DataFrame.from_dict(rag_dict)\n",
    "display(rag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Model with Reranking\n",
    "\n",
    "This model uses the provided documents with the provided model in a RAG system. Documents are assessed for vector similarity to the given prompt then reranked based on a second reranking model to provided more relevant source documents to use in the response. The response included both the answer to the given question and the document used to source the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_dict = []\n",
    "for question in QUESTIONS:\n",
    "    answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
    "    rank_dict.append({\n",
    "        \"prompt\": question,\n",
    "        \"response\": answer,\n",
    "        \"sources\": relevant_docs\n",
    "    })\n",
    "rank_df = pd.DataFrame.from_dict(rank_dict)\n",
    "display(rank_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
